"""
Model architectures: https://machinelearningmastery.com/keras-functional-api-deep-learning/

Best practices:
1. Consistent Variable Names. Use the same variable name for the input (visible) and output layers (output)
    and perhaps even the hidden layers (hidden1, hidden2). It will help to connect things together correctly.
2. Review Layer Summary. Always print the model summary and review the layer outputs to ensure that
    the model was connected together as you expected.
3. Review Graph Plots. Always create a plot of the model graph and review it to ensure that everything
    was put together as you intended.
4. Name the layers. You can assign names to layers that are used when reviewing summaries and plots of
    the model graph. For example: Dense(1, name=’hidden1′).
5. Separate Sub-models. Consider separating out the development of sub-models and combine the sub-models
    together at the end.
"""

# from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import *


def simple_model():
    """
    Simple starting model Template
    :return:
    """

    visible = Input(shape=(2,))
    hidden = Dense(2)(visible)

    model = Model(inputs=visible, outputs=hidden)

    return model


def multilayer_perceptron():
    """
    Multilayer Perceptron Template
    :return:
    """
    visible = Input(shape=(10,))

    hidden1 = Dense(10, activation='relu')(visible)
    hidden2 = Dense(20, activation='relu')(hidden1)
    hidden3 = Dense(10, activation='relu')(hidden2)

    output = Dense(1, activation='sigmoid')(hidden3)

    model = Model(inputs=visible, outputs=output)

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='multilayer_perceptron_graph.png')

    return model


def cnn_model():
    """
    Convolutional Neural Network Template
    :return:
    """

    visible = Input(shape=(64, 64, 1))

    conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    flat = Flatten()(pool2)

    hidden1 = Dense(10, activation='relu')(flat)
    output = Dense(1, activation='sigmoid')(hidden1)

    model = Model(inputs=visible, outputs=output)

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='convolutional_neural_network.png')


def rnn_model():
    """
    Recurrent Neural Network Template
    :return:
    """
    visible = Input(shape=(100, 1))

    hidden1 = LSTM(10)(visible)
    hidden2 = Dense(10, activation='relu')(hidden1)

    output = Dense(1, activation='sigmoid')(hidden2)

    model = Model(inputs=visible, outputs=output)

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='recurrent_neural_network.png')


def shared_layers():
    """
    Shared Input Layer Template
    :return:
    """
    # input layer
    visible = Input(shape=(64, 64, 1))

    # first feature extractor
    conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    flat1 = Flatten()(pool1)

    # second feature extractor
    conv2 = Conv2D(16, kernel_size=8, activation='relu')(visible)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    flat2 = Flatten()(pool2)

    # merge feature extractors
    merge = concatenate([flat1, flat2])

    # interpretation layer
    hidden1 = Dense(10, activation='relu')(merge)

    # prediction output
    output = Dense(1, activation='sigmoid')(hidden1)

    model = Model(inputs=visible, outputs=output)

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='shared_input_layer.png')


def shared_feature_extraction_layer():
    """
    Shared Feature Extraction Layer Template
    :return:
    """
    # define input
    visible = Input(shape=(100, 1))

    # feature extraction
    extract1 = LSTM(10)(visible)

    # first interpretation model
    interp1 = Dense(10, activation='relu')(extract1)

    # second interpretation model
    interp11 = Dense(10, activation='relu')(extract1)
    interp12 = Dense(20, activation='relu')(interp11)
    interp13 = Dense(10, activation='relu')(interp12)

    # merge interpretation
    merge = concatenate([interp1, interp13])

    # output
    output = Dense(1, activation='sigmoid')(merge)
    model = Model(inputs=visible, outputs=output)

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='shared_feature_extractor.png')


def multiple_input_model():
    """
    Multi Input Model Template
    :return:
    """
    # first input model
    visible1 = Input(shape=(64, 64, 1))
    conv11 = Conv2D(32, kernel_size=4, activation='relu')(visible1)
    pool11 = MaxPooling2D(pool_size=(2, 2))(conv11)
    conv12 = Conv2D(16, kernel_size=4, activation='relu')(pool11)
    pool12 = MaxPooling2D(pool_size=(2, 2))(conv12)
    flat1 = Flatten()(pool12)

    # second input model
    visible2 = Input(shape=(32, 32, 3))
    conv21 = Conv2D(32, kernel_size=4, activation='relu')(visible2)
    pool21 = MaxPooling2D(pool_size=(2, 2))(conv21)
    conv22 = Conv2D(16, kernel_size=4, activation='relu')(pool21)
    pool22 = MaxPooling2D(pool_size=(2, 2))(conv22)
    flat2 = Flatten()(pool22)

    # merge input models
    merge = concatenate([flat1, flat2])

    # interpretation model
    hidden1 = Dense(10, activation='relu')(merge)
    hidden2 = Dense(10, activation='relu')(hidden1)
    output = Dense(1, activation='sigmoid')(hidden2)
    model = Model(inputs=[visible1, visible2], outputs=output)

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='multiple_inputs.png')


def mulitple_outputs_model():
    """
    Multiple output model Template
    :return:
    """
    # input layer
    visible = Input(shape=(100, 1))

    # feature extraction
    extract = LSTM(10, return_sequences=True)(visible)

    # classification output
    class11 = LSTM(10)(extract)
    class12 = Dense(10, activation='relu')(class11)
    output1 = Dense(1, activation='sigmoid')(class12)

    # sequence output
    output2 = TimeDistributed(Dense(1, activation='linear'))(extract)

    # output
    model = Model(inputs=visible, outputs=[output1, output2])

    # summarize layers
    print(model.summary())

    # plot graph
    # plot_model(model, to_file='multiple_outputs.png')
